{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наивный гауссовский классификатор\n",
    "\n",
    "Наивные байесовские классификаторы основаны на байесовских методах классификации, в основе которых лежит теорема Байеса — уравнение, описывающее \n",
    "связь условных вероятностей статистических величин. В байесовской классификации нас интересует поиск вероятности метки (категории) при определенных заданных признаках, являющихся результатами наблюдений/экспериментов величин.\n",
    " \n",
    "В  наивном гауссовском классификаторе мы принимаем допущение, что правдоподобие признаковых значений х при условии, что наблюдение принадлежит классу у, подчиняется нормальному распределению."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n"
     ]
    }
   ],
   "source": [
    "# Загрузить библиотеки \n",
    "from sklearn import datasets\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "# Загрузить данные\n",
    "wine = datasets.load_wine()\n",
    "features = wine.data \n",
    "target = wine.target\n",
    "# Создать объект наивного гауссовского классификатора \n",
    "clf = GaussianNB()\n",
    "# Натренировать модель\n",
    "model = clf.fit(features, target)\n",
    "print(clf.predict(features[2:3]))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В библиотеке scikit-learn мы тренируем Наивный гауссовский классификатор, используя метод fit, и, в свою очередь, можем делать предсказания \n",
    "о классе наблюдения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создать новое наблюдение \n",
    "new_observation = [[ 4, 4, 4, 0.4,4,4,4,4,0.4,0.4,4,4,4]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Делаем предсказание о классе наблюдений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Предсказать класс\n",
    "model.predict(new_observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Правильность на тестовом наборе: 0.99\n"
     ]
    }
   ],
   "source": [
    "print(\"Правильность на тестовом наборе: {:.2f}\".format(clf.score(features, target)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n"
     ]
    }
   ],
   "source": [
    "# Создать объект наивного  гауссового классификатора с априорными вероятностями для каждого класса \n",
    "сlf = GaussianNB(priors=[0.25, 0.25, 0.5])\n",
    "# Натренировать модель\n",
    "model = clf.fit(features, target)\n",
    "print(clf.predict(features[2:3]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "для дискретных и счетных признаков будем использовать полиномиальный наивный байесов классификатор "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузить библиотеки \n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "# Загрузить данные\n",
    "wine = datasets.load_wine()\n",
    "features = wine.data \n",
    "target = wine.target\n",
    "# Создать объект полиномиального наивного байесова классификатора  с априорными вероятностями каждого класса \n",
    "clf = MultinomialNB()\n",
    "# Натренировать модель\n",
    "model = clf.fit(features, target)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В классе MuitinoniaiNB модели тренируются с использованием метода fit, а наблюдения можно предсказать с помощью метода predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создать новое наблюдение \n",
    "new_observation = [[4, 4, 4, 0.4, 4, 4, 4, 4, 0.4, 0.4, 4, 4, 4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Предсказать класс нового наблюдения \n",
    "model.predict(new_observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Правильность на тестовом наборе: 0.87\n"
     ]
    }
   ],
   "source": [
    "print(\"Правильность на тестовом наборе: {:.2f}\".format(clf.score(features, target)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если параметр ciass_prior не задан, то априорные вероятности заучиваются на основе данных.\n",
    "Если мы хотим, чтобы в качестве априорного распределения использовалась равномерное, то можно задать fit_prior=False.\n",
    "Класс MuitinomiaiNB содержит гиперпараметр аддитивного сглаживания alpha, который должен быть настроен. \n",
    "Значение по умолчанию равняется 1.0, при этом значение о.о означает отсутствие сглаживания."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Бернуллиев наивный байесов классификатор или наивный байесова классификатор для бинарных признаков \n",
    "\n",
    "\n",
    "Принимаем допущение, что все наши признаки являются бинарными, т. е. принимают только два значения. \n",
    "Кроме того, класс BernoulliNB имеет гиперпараметр аддитивного сглаживания alpha, который требуется настроить с помощью методов отбора модели. Наконец, если необходимо использовать априорные вероятности, то можно применить параметр class prior со списком, содержащим априорные\n",
    "вероятности для каждого класса. Если требуется указать равномерную априорную\n",
    "вероятности, то его можно указать  fit_prior=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузить библиотеки \n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "# Создать три бинарных признака\n",
    "features = np.random.randint(2, size=(100, 3))\n",
    "# Создать вектор бинарных целей\n",
    "target = np.random.randint(2, size=(100, 1)).ravel()\n",
    "# Создать объект бернуллиева наивного Байеса с априорными вероятностями каждого класса \n",
    "clf = BernoulliNB(class_prior=[0.25, 0.5])\n",
    "# Натренировать модель\n",
    "model = clf.fit(features, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создать новое наблюдение \n",
    "new_observation = [[0, 0, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Предсказать класс нового наблюдения \n",
    "model.predict(new_observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Правильность на тестовом наборе: 0.87\n"
     ]
    }
   ],
   "source": [
    "print(\"Правильность на тестовом наборе: {:.2f}\".format(clf.score(features, target)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k ближайших соседей\n",
    "\n",
    "В простейшем варианте алгоритм k ближайших соседей рассматривает лишь одного ближайшего соседа – точку обучающего набора, ближе всего расположенную к точке, для которой мы хотим получить прогноз. \n",
    "Прогнозом является ответ, уже известный для данной точки обучающего набора."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Загружаем данные\n",
    "from sklearn.datasets import load_iris\n",
    "iris_dataset = load_iris()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В библиотеке scikit-learn есть функция train_test_split, которая перемешивает набор данных и разбивает его на две части. Эта функция отбирает в обучающий набор 75% строк данных с соответствующими метками. Оставшиеся 25% данных с метками объявляются тестовым набором. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для наших данных зададим обучающие данные, обучающие метки, тестовые данные, тестовые метки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split( iris_dataset['data'], iris_dataset['target'], random_state=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Алгоритм классификации на основе метода k ближайших соседей реализован в классификаторе KNeighborsClassifier модуля neighbors. Прежде чем использовать эту модель, нам нужно создать объект-экземпляр класса.\n",
    "Это произойдет, когда мы зададим параметры модели. Самым важным параметром KNeighborsClassifier является количество соседей, которые мы установим равным 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для построения модели на обучающем наборе, мы вызываем метод fit объекта knn, который принимает в качестве аргументов массив \n",
    "NumPy X_train, содержащий обучающие данные, и массив NumPy y_train, соответствующий обучающим меткам:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=10)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Представим, что мы нашли в дикой природе ирис с длиной чашелистика 5 см, шириной чашелистика 2.9 см, длиной лепестка 1 см и шириной лепестка 0.2 см. К какому сорту ириса нужно отнести этот цветок\n",
    "Мы можем поместить эти данные в массив NumPy, снова вычисляя форму массива, т.е. количество примеров (1), умноженное на количество признаков (4):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "форма массива X_new: (1, 4)\n",
      "Прогноз: [0]\n",
      "Спрогнозированная метка: ['setosa']\n"
     ]
    }
   ],
   "source": [
    "#Занесем параметры найденного цветка в массив \n",
    "X_new = np.array([[5, 2.9, 1, 0.2]])\n",
    "print(\"форма массива X_new: {}\".format(X_new.shape))\n",
    "#Делаем прогноз\n",
    "prediction = knn.predict(X_new)\n",
    "print(\"Прогноз: {}\".format(prediction))\n",
    "print(\"Спрогнозированная метка: {}\".format( iris_dataset['target_names'][prediction]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Правильность на тестовом наборе: 0.97\n"
     ]
    }
   ],
   "source": [
    "print(\"Правильность на тестовом наборе: {:.2f}\".format(knn.score(X_test, y_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c14266d1cf48f2d959a70813fdeb285e0a590ed60b00c7fdad4a41e41adcb60b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
